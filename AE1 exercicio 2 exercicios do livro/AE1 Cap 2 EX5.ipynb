{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f38217b-ece7-44a0-91a8-b428890c5b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7b13e3b-5236-4dc1-b56f-475df368afe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)  # Generate the data.\n",
    "xTrain = np.array([-1.0, -0.8, -0.2, 0.5, 0.7])\n",
    "yTrain = -(xTrain ** 2) + 10.0\n",
    "yTrain[3] -= 0.2\n",
    "xTest = np.array([-1.1, -0.6, 0.1, 0.3])\n",
    "yTest = -(xTest ** 2) + 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "033f182b-50b5-4f3e-abab-8612d57fe043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class HigherOrderRegression:\n",
    "    def __init__(self, polynomialDegree, regularization=None):\n",
    "        self.polynomialDegree = int(polynomialDegree)\n",
    "        self.regularization = regularization\n",
    "        self.weights = np.zeros(self.polynomialDegree, dtype=float)\n",
    "        self.bias = 0.0\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = np.asarray(x, dtype=float).reshape(-1)          # (N,)\n",
    "        y = np.full_like(x, fill_value=self.bias, dtype=float)\n",
    "        for i in range(self.polynomialDegree):\n",
    "            y += self.weights[i] * (x ** (i + 1))\n",
    "        return y                                            # (N,)\n",
    "\n",
    "    # alias útil\n",
    "    def predict(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def costFunction(self, x, y):\n",
    "        x = np.asarray(x, dtype=float).reshape(-1)\n",
    "        y = np.asarray(y, dtype=float).reshape(-1)\n",
    "        err = self.forward(x) - y\n",
    "        cost = np.mean(err ** 2)\n",
    "        if self.regularization is not None:\n",
    "            cost += self.regularization * (np.sum(self.weights ** 2) + self.bias ** 2)\n",
    "        return cost\n",
    "\n",
    "    def gradient(self, x, y):\n",
    "        x = np.asarray(x, dtype=float).reshape(-1)\n",
    "        y = np.asarray(y, dtype=float).reshape(-1)\n",
    "        err = self.forward(x) - y                           # (N,)\n",
    "        grad_b = 2 * np.mean(err)\n",
    "        grad_w = np.zeros(self.polynomialDegree, dtype=float)\n",
    "        for i in range(self.polynomialDegree):\n",
    "            grad_w[i] = 2 * np.mean(err * (x ** (i + 1)))\n",
    "            if self.regularization is not None:\n",
    "                grad_w[i] += 2 * self.regularization * self.weights[i]\n",
    "        # Obs.: não regularizamos o bias (como no seu comentário)\n",
    "        return grad_w, grad_b\n",
    "\n",
    "    def train(self, epochs, lr, xTrain, yTrain, xTest=None, yTest=None, verbose=100):\n",
    "        for epoch in range(epochs):\n",
    "            costTrain = self.costFunction(xTrain, yTrain)\n",
    "            costTest = self.costFunction(xTest, yTest) if (xTest is not None and yTest is not None) else np.nan\n",
    "\n",
    "            grad_w, grad_b = self.gradient(xTrain, yTrain)\n",
    "            self.weights -= lr * grad_w\n",
    "            self.bias    -= lr * grad_b\n",
    "\n",
    "            if verbose and (epoch % verbose == 0 or epoch == epochs - 1):\n",
    "                print(f\"Epoch: {epoch}/{epochs}\\tTraining cost = {costTrain:.2e}\\tValidation cost = {costTest:.2e}\")\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "019a1c3d-90c3-4fb4-a02f-7e0f6eff3089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/1000\tTraining cost = 8.99e+01\tValidation cost = 9.21e+01\n",
      "Epoch: 100/1000\tTraining cost = 1.10e-01\tValidation cost = 5.65e-01\n",
      "Epoch: 200/1000\tTraining cost = 3.66e-02\tValidation cost = 2.21e-01\n",
      "Epoch: 300/1000\tTraining cost = 1.38e-02\tValidation cost = 7.67e-02\n",
      "Epoch: 400/1000\tTraining cost = 6.19e-03\tValidation cost = 3.38e-02\n",
      "Epoch: 500/1000\tTraining cost = 3.46e-03\tValidation cost = 2.35e-02\n",
      "Epoch: 600/1000\tTraining cost = 2.34e-03\tValidation cost = 2.31e-02\n",
      "Epoch: 700/1000\tTraining cost = 1.78e-03\tValidation cost = 2.53e-02\n",
      "Epoch: 800/1000\tTraining cost = 1.44e-03\tValidation cost = 2.80e-02\n",
      "Epoch: 900/1000\tTraining cost = 1.18e-03\tValidation cost = 3.06e-02\n",
      "Epoch: 999/1000\tTraining cost = 9.83e-04\tValidation cost = 3.30e-02\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.HigherOrderRegression at 0x78639a3d33a0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polynomialDegree = 9\n",
    "lr = 1e-1\n",
    "epochs = 1000\n",
    "regularization = 1e-2\n",
    "\n",
    "model = HigherOrderRegression(polynomialDegree)\n",
    "# model = HigherOrderRegression(polynomialDegree, regularization) # activate the regularization\n",
    "model.train(epochs, lr, xTrain, yTrain, xTest, yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659044e9-146a-4376-8d9c-a4059b52f06a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-ai-2024.04-py310",
   "language": "python",
   "name": "conda-env-anaconda-ai-2024.04-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
